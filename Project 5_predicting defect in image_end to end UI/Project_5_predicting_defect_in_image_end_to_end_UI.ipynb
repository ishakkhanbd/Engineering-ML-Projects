{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CI_54aNZkDc",
        "outputId": "5eb5ec7f-3631-4afe-c238-a932f0a8f005"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset created with labels.csv\n"
          ]
        }
      ],
      "source": [
        "#Generating a dataset of 200 images. You can use your own dataset.\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "\n",
        "# Set directories\n",
        "os.makedirs(\"dataset/images\", exist_ok=True)\n",
        "\n",
        "labels = []\n",
        "file_names = []\n",
        "\n",
        "# Generate 100 black images (no defect)\n",
        "for i in range(100):\n",
        "    img = np.zeros((128, 128), dtype=np.uint8)  # black image\n",
        "    file_name = f\"dataset/images/no_defect_{i}.png\"\n",
        "    Image.fromarray(img).save(file_name)\n",
        "    file_names.append(file_name)\n",
        "    labels.append(\"no_defect\")\n",
        "\n",
        "# Generate 100 images with random white scratches (defect)\n",
        "for i in range(100):\n",
        "    img = np.zeros((128, 128), dtype=np.uint8)\n",
        "    num_scratches = np.random.randint(1, 6)\n",
        "    for _ in range(num_scratches):\n",
        "        # Randomly choose orientation and position\n",
        "        orientation = np.random.choice([\"horizontal\", \"vertical\"])\n",
        "        if orientation == \"horizontal\":\n",
        "            y = np.random.randint(0, 128)\n",
        "            x_start = np.random.randint(0, 100)\n",
        "            x_end = x_start + np.random.randint(5, 28)\n",
        "            img[y, x_start:x_end] = 255\n",
        "        else:\n",
        "            x = np.random.randint(0, 128)\n",
        "            y_start = np.random.randint(0, 100)\n",
        "            y_end = y_start + np.random.randint(5, 28)\n",
        "            img[y_start:y_end, x] = 255\n",
        "    file_name = f\"dataset/images/defect_{i}.png\"\n",
        "    Image.fromarray(img).save(file_name)\n",
        "    file_names.append(file_name)\n",
        "    labels.append(\"defect\")\n",
        "\n",
        "# Create dataframe\n",
        "df = pd.DataFrame({\"file_name\": file_names, \"label\": labels})\n",
        "df.to_csv(\"dataset/labels.csv\", index=False)\n",
        "print(\"Dataset created with labels.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Training images by baseline CNN\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "# Load dataframe\n",
        "df = pd.read_csv(\"dataset/labels.csv\")\n",
        "\n",
        "# Encode labels\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "le = LabelEncoder()\n",
        "df['label_enc'] = le.fit_transform(df['label'])  # 0=no_defect, 1=defect\n",
        "\n",
        "# Split dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label_enc'])\n",
        "\n",
        "# Image data generator\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_gen = train_datagen.flow_from_dataframe(\n",
        "    train_df,\n",
        "    x_col='file_name',\n",
        "    y_col='label',\n",
        "    target_size=(128,128),\n",
        "    color_mode='grayscale',\n",
        "    class_mode='binary',\n",
        "    batch_size=16,\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_gen = val_datagen.flow_from_dataframe(\n",
        "    val_df,\n",
        "    x_col='file_name',\n",
        "    y_col='label',\n",
        "    target_size=(128,128),\n",
        "    color_mode='grayscale',\n",
        "    class_mode='binary',\n",
        "    batch_size=16,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Baseline CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(16, (3,3), activation='relu', input_shape=(128,128,1)),\n",
        "    MaxPooling2D(2,2),\n",
        "    Conv2D(32, (3,3), activation='relu'),\n",
        "    MaxPooling2D(2,2),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(train_gen, validation_data=val_gen, epochs=5)\n",
        "\n",
        "# Save model\n",
        "model.save(\"scratch_detector_cnn.h5\")\n",
        "\n",
        "# Save label encoder\n",
        "with open(\"label_encoder.pkl\", \"wb\") as f:\n",
        "    pickle.dump(le, f)\n",
        "\n",
        "print(\"Model and label encoder saved!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OVuYRUXqaBQA",
        "outputId": "3aa6a244-a64a-408b-af99-86745a752ee4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 160 validated image filenames belonging to 2 classes.\n",
            "Found 40 validated image filenames belonging to 2 classes.\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 251ms/step - accuracy: 0.4721 - loss: 0.6754 - val_accuracy: 0.5000 - val_loss: 0.5346\n",
            "Epoch 2/5\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 228ms/step - accuracy: 0.6974 - loss: 0.4823 - val_accuracy: 1.0000 - val_loss: 0.3757\n",
            "Epoch 3/5\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 257ms/step - accuracy: 1.0000 - loss: 0.3499 - val_accuracy: 1.0000 - val_loss: 0.3060\n",
            "Epoch 4/5\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 228ms/step - accuracy: 1.0000 - loss: 0.2558 - val_accuracy: 1.0000 - val_loss: 0.1944\n",
            "Epoch 5/5\n",
            "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 194ms/step - accuracy: 1.0000 - loss: 0.1305 - val_accuracy: 0.9500 - val_loss: 0.0984\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model and label encoder saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now, you can save the .pkl file and .h5 file and run the app.py to run the UI and predict defect in an unseen test image."
      ],
      "metadata": {
        "id": "dY9038v3b5Dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fcmQVbl0b5GA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}